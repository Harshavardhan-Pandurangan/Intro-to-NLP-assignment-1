{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC\n",
    "# import tkr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_estimator(self):\n",
    "    Nr = {}\n",
    "    for _, freq in self.ngram_freq.items():\n",
    "        if freq in Nr:\n",
    "            Nr[freq] += 1\n",
    "        else:\n",
    "            Nr[freq] = 1\n",
    "\n",
    "    Zr = {}\n",
    "    Zr[1] = Nr[1]\n",
    "\n",
    "    Nr_items = sorted(Nr.items())\n",
    "    # print(Nr_items)\n",
    "\n",
    "    for i, thing in enumerate(Nr_items[1:-1], start=1):\n",
    "        Zr[thing[0]] = (thing[1] * 2) / (Nr_items[i+1][0] - Nr_items[i-1][0])\n",
    "\n",
    "    # print(sorted(Zr.items()))\n",
    "    X = [[log(i[0])] for i in Zr.items()]\n",
    "    y = [log(i[1]) for i in Zr.items()]\n",
    "\n",
    "    md = LinearRegression()\n",
    "    md.fit(X, y)\n",
    "\n",
    "    self.Zr = Zr\n",
    "    self.md = md\n",
    "\n",
    "    # For each value in Zr, compute Turing Estimate directly, and also using the model\n",
    "    # Compute the variance using the known Zr values\n",
    "    swap_value = 0\n",
    "\n",
    "    estimates = []\n",
    "\n",
    "    for r in range(1, max(Zr)):\n",
    "        try:\n",
    "            turing_estimate = (r + 1) * (Nr[r+1]) / Nr[r]\n",
    "\n",
    "            estimates.append(turing_estimate)\n",
    "\n",
    "            pred_logs = md.predict([[log(r+1)], [log(r)]])\n",
    "            pred_estimate = (r + 1) * (exp(pred_logs[0])) / exp(pred_logs[1])\n",
    "\n",
    "            variance = ((r + 1) ** 2) * (Nr[r+1] / (Nr[r] ** 2)) * (1 + ((Nr[r+1] / (Nr[r]))))\n",
    "\n",
    "            if abs(turing_estimate - pred_estimate) < GT_CONFIDENCE * (variance ** 0.5):\n",
    "                swap_value = r\n",
    "                break\n",
    "\n",
    "        except KeyError:\n",
    "            swap_value = r\n",
    "            break\n",
    "\n",
    "    # print(swap_value)\n",
    "\n",
    "    for r in range(swap_value, max(Zr)):\n",
    "        pred_logs = md.predict([[log(r+1)], [log(r)]])\n",
    "        pred_estimate = (r + 1) * (exp(pred_logs[0])) / exp(pred_logs[1])\n",
    "\n",
    "        estimates.append(pred_estimate)\n",
    "\n",
    "    # self.swap_value = swap_value\n",
    "    # print(len(estimates))\n",
    "    # print(Nr_items)\n",
    "    self.estimates = estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tkr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mLM_Base\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mABC\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtkr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize_string\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mN\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m, in \u001b[0;36mLM_Base\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mLM_Base\u001b[39;00m(ABC):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, N, text, tokenizer_function\u001b[38;5;241m=\u001b[39m\u001b[43mtkr\u001b[49m\u001b[38;5;241m.\u001b[39mtokenize_string):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mN \u001b[38;5;241m=\u001b[39m N\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer_function \u001b[38;5;241m=\u001b[39m tokenizer_function\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tkr' is not defined"
     ]
    }
   ],
   "source": [
    "class LM_Base(ABC):\n",
    "    def __init__(self, N, text, tokenizer_function=tkr.tokenize_string):\n",
    "        self.N = N\n",
    "        self.tokenizer_function = tokenizer_function\n",
    "        self.document = self.tokenizer_function(text)\n",
    "\n",
    "        self.train_doc = None\n",
    "        self.test_doc = None\n",
    "\n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        Given the document, compute the ngram_frequencies\n",
    "\n",
    "        \"\"\"\n",
    "        # self.document = self.tokenizer_function(text)\n",
    "        # For each token in the document, add N of the tokens to a tuple\n",
    "\n",
    "        if self.train_doc is not None:\n",
    "            # Use train_doc, else use the entire document.\n",
    "            self.document = self.train_doc\n",
    "\n",
    "\n",
    "        ngrams = []\n",
    "        for sentence in self.document:\n",
    "\n",
    "            sentence.insert(0, '<s>')\n",
    "            sentence.append('</s>')\n",
    "\n",
    "            for i in range(len(sentence)):\n",
    "                n_window = sentence[i:i+self.N]\n",
    "                if len(n_window) == self.N:\n",
    "                    ngrams.append(tuple(n_window))\n",
    "\n",
    "        ngram_freq = {}\n",
    "        for ngram in ngrams:\n",
    "            if ngram in ngram_freq:\n",
    "                ngram_freq[ngram] += 1\n",
    "            else:\n",
    "                ngram_freq[ngram] = 1\n",
    "\n",
    "        self.ngrams = ngrams\n",
    "        self.ngram_freq = ngram_freq\n",
    "\n",
    "    @abstractmethod\n",
    "    def ngram_estimator(self, ngram):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def setup_estimator(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "    def train_test_split(self, test_size=1000, seed=None):\n",
    "        if self.document is None:\n",
    "            raise Exception('Model not fitted yet')\n",
    "        else:\n",
    "            if seed is not None:\n",
    "                random.seed(seed)\n",
    "            random.shuffle(self.document)\n",
    "\n",
    "            self.train_doc = self.document[:-test_size]\n",
    "            self.test_doc = self.document[-test_size:]\n",
    "\n",
    "        return self.train_doc, self.test_doc\n",
    "\n",
    "    def perplexity(self, sentence):\n",
    "        if self.document is None:\n",
    "            raise Exception('Model not fitted yet')\n",
    "\n",
    "        if type(sentence) == str:  # Tokenize and add beginning, end of sentence tags.\n",
    "            sents = self.tokenizer_function(sentence)\n",
    "            for st in sents:\n",
    "                st.insert(0, '<s>')\n",
    "                st.append('</s>')\n",
    "\n",
    "        elif type(sentence) == list:  # If sentence is already tokenized.\n",
    "            sents = sentence\n",
    "\n",
    "        n = len(sents[0])\n",
    "\n",
    "        pred_score = self.predict(sentence)\n",
    "        return pred_score ** (-1/n)\n",
    "\n",
    "\n",
    "    def predict(self, sentence):\n",
    "        if type(sentence) == str:  # Tokenize and add beginning, end of sentence tags.\n",
    "            sents = self.tokenizer_function(sentence)\n",
    "            for st in sents:\n",
    "                st.insert(0, '<s>')\n",
    "                st.append('</s>')\n",
    "\n",
    "        elif type(sentence) == list:  # If sentence is already tokenized.\n",
    "            sents = sentence\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Sentence must be a string or a list of tokens.\")\n",
    "\n",
    "        sent_ngrams = []\n",
    "        for sentence in sents:\n",
    "            for i in range(len(sentence)):\n",
    "                n_window = sentence[i:i+self.N]\n",
    "                if len(n_window) == self.N:\n",
    "                    sent_ngrams.append(tuple(n_window))\n",
    "\n",
    "        sent_prob = 1\n",
    "        for ngram in sent_ngrams:\n",
    "            sent_prob *= self.ngram_estimator(ngram)\n",
    "            # print(sent_prob, self.ngram_freq.get(ngram, 0))\n",
    "\n",
    "        return sent_prob\n",
    "\n",
    "    def generate(self, sentence, k:int):\n",
    "        if type(sentence) == str:  # Tokenize and add beginning, end of sentence tags.\n",
    "            sents = self.tokenizer_function(sentence)\n",
    "            for st in sents:\n",
    "                st.insert(0, '<s>')\n",
    "                st.append('</s>')\n",
    "\n",
    "        elif type(sentence) == list:  # If sentence is already tokenized.\n",
    "            sents = sentence\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Sentence must be a string or a list of tokens.\")\n",
    "\n",
    "        last_sent = sents[-1]\n",
    "        last_window = last_sent[- self.N :-1]\n",
    "\n",
    "        possible_ngrams = []\n",
    "        for ngram in self.ngrams:\n",
    "            for i, token in enumerate(last_window):\n",
    "                if ngram[i] != token:\n",
    "                    break\n",
    "            else:\n",
    "                if ngram not in possible_ngrams:\n",
    "                    possible_ngrams.append(ngram)\n",
    "\n",
    "        outputs = {ng: self.predict([' '.join(ng)]) / self.predict(' '.join(last_sent[- self.N:-1])) for ng in possible_ngrams}\n",
    "        # for ng in possible_ngrams:\n",
    "        #    print(ng, self.predict([' '.join(ng)]), self.predict(' '.join(last_sent[- self.N:-1])))\n",
    "        # print(sorted(outputs.items(), key=lambda x: x[1], reverse=True))\n",
    "        return sorted(outputs.items(), key=lambda x: x[1], reverse=True)[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LM_Base' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mLM_Interpolation\u001b[39;00m(\u001b[43mLM_Base\u001b[49m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, N, text, tokenizer_function\u001b[38;5;241m=\u001b[39mtkr\u001b[38;5;241m.\u001b[39mtokenize_string):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(N, text, tokenizer_function)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LM_Base' is not defined"
     ]
    }
   ],
   "source": [
    "class LM_Interpolation(LM_Base):\n",
    "    def __init__(self, N, text, tokenizer_function=tkr.tokenize_string):\n",
    "        super().__init__(N, text, tokenizer_function)\n",
    "\n",
    "        self.all_estimators = None\n",
    "        self.lambdas = None\n",
    "\n",
    "        unigrams = []\n",
    "        for sentence in self.document:\n",
    "            for i in range(len(sentence)):\n",
    "                n_window = sentence[i:i+1]\n",
    "                if len(n_window) == 1:\n",
    "                    unigrams.append(tuple(n_window))\n",
    "\n",
    "        unigram_freq = {}\n",
    "        for unigram in unigrams:\n",
    "            if unigram not in unigram_freq:\n",
    "                unigram_freq[unigram] = 1\n",
    "            else:\n",
    "                unigram_freq[unigram] += 1\n",
    "\n",
    "        freq_of_freqs = {}\n",
    "        for n, c in unigram_freq.items():\n",
    "            if c not in freq_of_freqs:\n",
    "                freq_of_freqs[c] = 1\n",
    "            else:\n",
    "                freq_of_freqs[c] += 1\n",
    "\n",
    "        held_out_vocabulary = []\n",
    "        for ng in unigram_freq:\n",
    "            if unigram_freq[ng] == 1 and random.randint(1, 10) > 7:\n",
    "                held_out_vocabulary.append(ng[0])\n",
    "\n",
    "        for i in range(len(self.document)):\n",
    "            for j in range(len(self.document[i])):\n",
    "                if self.document[i][j] in held_out_vocabulary:\n",
    "                    self.document[i][j] = '<UNK>'\n",
    "\n",
    "        self.held_out_vocabulary = held_out_vocabulary\n",
    "\n",
    "    def ngram_estimator(self, ngram):\n",
    "        if self.ngrams is None:\n",
    "            raise Exception('Model not fitted yet.')\n",
    "\n",
    "        cleaned_ngram = []\n",
    "        for token in ngram:\n",
    "            if token in self.held_out_vocabulary:\n",
    "                cleaned_ngram.append('<UNK>')\n",
    "            if self.all_estimators[1].get((token, )) is None:\n",
    "                cleaned_ngram.append('<UNK>')\n",
    "            else:\n",
    "                cleaned_ngram.append(token)\n",
    "        cleaned_ngram = tuple(cleaned_ngram)\n",
    "\n",
    "        if self.N != 3:\n",
    "            raise NotImplementedError(\"Interpolation model only implemented for trigrams.\")\n",
    "\n",
    "        try:\n",
    "            p3 = self.all_estimators[3].get(cleaned_ngram, 0) / self.all_estimators[2].get(cleaned_ngram[:-1], 0)\n",
    "        except ZeroDivisionError:\n",
    "            p3 = 0\n",
    "\n",
    "        try:\n",
    "            p2 = self.all_estimators[2].get(cleaned_ngram[1:],0) / self.all_estimators[1].get(cleaned_ngram[1:-1], 0)\n",
    "        except ZeroDivisionError:\n",
    "            p2 = 0\n",
    "\n",
    "        try:\n",
    "            p1 = self.all_estimators[1].get(cleaned_ngram[-1:], 0) / sum([v for k,v in self.all_estimators[1].items()])\n",
    "        except ZeroDivisionError:\n",
    "            p1 = 0\n",
    "\n",
    "        p = (self.lambdas[1] * p1) + (self.lambdas[2] * p2) + (self.lambdas[3] * p3)\n",
    "\n",
    "        # print(p)\n",
    "\n",
    "        return (self.lambdas[1] * p1) + (self.lambdas[2] * p2) + (self.lambdas[3] * p3)\n",
    "\n",
    "\n",
    "    def setup_estimator(self):\n",
    "        if self.ngrams is None:\n",
    "            raise Exception('Model not fitted yet.')\n",
    "\n",
    "        if self.train_doc is not None:\n",
    "            self.document = self.train_doc\n",
    "\n",
    "        self.all_estimators = {}\n",
    "        self.all_estimators[self.N] = self.ngram_freq\n",
    "        # Count for all smaller windows\n",
    "        for window in range(1, self.N):\n",
    "            ngrams = []\n",
    "            for sentence in self.document:\n",
    "                for i in range(len(sentence)):\n",
    "                    n_window = sentence[i:i+window]\n",
    "                    if len(n_window) == window:\n",
    "                        ngrams.append(tuple(n_window))\n",
    "\n",
    "            ngram_freq = {}\n",
    "            for ngram in ngrams:\n",
    "                if ngram in ngram_freq:\n",
    "                    ngram_freq[ngram] += 1\n",
    "                else:\n",
    "                    ngram_freq[ngram] = 1\n",
    "\n",
    "            self.all_estimators[window] = ngram_freq\n",
    "\n",
    "        if self.N != 3:\n",
    "            raise NotImplementedError(\"Interpolation works for N=3, not others.\")\n",
    "\n",
    "        lambdas = {i:0 for i in range(1, self.N+1)}\n",
    "        normalizing_factor = 0\n",
    "\n",
    "        for ng in self.ngram_freq:\n",
    "            try:\n",
    "                l3_checker = (self.all_estimators[3][ng] - 1) / (self.all_estimators[2][ng[:2]] - 1)\n",
    "            except ZeroDivisionError:\n",
    "                l3_checker = 0\n",
    "\n",
    "            try:\n",
    "                l2_checker = (self.all_estimators[2][ng[1:]] - 1) / (self.all_estimators[1][ng[1:2]] - 1)\n",
    "            except ZeroDivisionError:\n",
    "                l2_checker = 0\n",
    "\n",
    "            try:\n",
    "                l1_checker = (self.all_estimators[1][ng[-1:]] - 1) / (len(self.ngrams) - 1)\n",
    "            except ZeroDivisionError:\n",
    "                l1_checker = 0\n",
    "\n",
    "\n",
    "            max_check = max([l1_checker, l2_checker, l3_checker])\n",
    "\n",
    "            if max_check == l3_checker:\n",
    "                lambdas[3] += self.ngram_freq[ng]\n",
    "\n",
    "            elif max_check == l2_checker:\n",
    "                lambdas[2] += self.ngram_freq[ng]\n",
    "\n",
    "            else:\n",
    "                lambdas[1] += self.ngram_freq[ng]\n",
    "\n",
    "            normalizing_factor += self.ngram_freq[ng]\n",
    "\n",
    "        lambdas = {k: lambdas[k] / normalizing_factor for k in lambdas}\n",
    "\n",
    "        self.lambdas = lambdas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
